
Can I create a Large Language Model from scratch?

- Source of study [Build a Large Language Model - Sebastian Raschka](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167)

- Video Reference: [Build an LLM from scratch](https://www.youtube.com/watch?v=kPGTx4wcm_w)



## What to Learn Here so far

My journey learning how Transformers working to build a Large Language Model. Describing the process backwards

- Using an text entry, define a tokenization process that will be used for training
- Generate Embedding representations of tokens
- Recreate the multiple layers of the Transformer Architecture
- Create the Attention Mechanism that is the most important aspect of the LLM 'revolution'
- Train a neural network model using the Transformer Architecture 
- Load a Model and execute the Text Generation Process using the class `GPTModel` 
- Use the GPT2 open weights to load the models in our own architecture
- Alter the Transformer Architecture, allowing for Fine tuning of a pre trained model
- Create a SpamClassifier that's capable of evaluate a message to say if it's spam or not
- Create a Instructions Dataset and Dataloader to finetune another model
- Accessing local ollama instance for testing
- Evaluate the results of my model using ollama

ðŸ”¥ðŸ”¥ðŸ”¥ That's a wrap! 


You can also refer to my notes, taken using obsidian during the process (not curated - just brain dump from my sessions):

- [LLM from Scratch - notes](LLM%20from%20Scratch.pdf)


