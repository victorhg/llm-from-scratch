{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "In order to evaluate how good a model is we need to think in percentages: \n",
    "- \"On average, the model responds approprietly to questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myllm.gpt as gpt\n",
    "import myllm.util\n",
    "import myllm.data as data\n",
    "import json\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP OVER MODEL\n",
    "\n",
    "\n",
    "# load model\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "gpt_config = myllm.util.gpt_config(\"gpt2-medium\")\n",
    "gpt_config.update({'qkv_bias': True})\n",
    "model = gpt.GPTModel(gpt_config)\n",
    "\n",
    "# fine tuned model\n",
    "model_state_dict = torch.load(\"model_instruction.pth\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# load dataset \n",
    "train_data, val_data, test_data = data.split_instruction_data(\"data/instruction-data.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [1:59:29<00:00, 65.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# We iterate over the entire trest set to validate answers\n",
    "from tqdm import tqdm \n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = data.format_instruction_input(entry)\n",
    "\n",
    "    token_ids = model.generate(\n",
    "        idx=myllm.util.text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=gpt_config[\"context_length\"],\n",
    "        eos_id=data.PAD_TOKEN_ID,\n",
    "    ).to(device)\n",
    "\n",
    "    generated_text = myllm.util.token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': ''}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_data[0])\n",
    "with open(\"data/instruction-data.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save information\n",
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', '355M')}-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Ollama \n",
    "\n",
    "in order to validade we need another llm to work with us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama?  True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\n",
    "        \"Ollama not running. Launch ollama\"\n",
    "    )\n",
    "\n",
    "print(\"Ollama? \", check_if_running(\"ollama\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "\n",
    "def query_model(prompt, model=\"llama3.2:latest\", \n",
    "                url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model, \n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\", \"content\": prompt\n",
    "        }],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        } \n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request( \n",
    "        url, data=payload, method=\"POST\"\n",
    "    )\n",
    "\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urllib.request.Request object at 0x149cf4bd0>\n",
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and grassy weeds.\n",
      "2. Hay: High-quality hay, such as timothy hay or alfalfa hay, is a staple in a llama's diet. It provides essential nutrients like fiber, protein, and vitamins.\n",
      "3. Grains: Llamas may also be fed grains like oats, barley, or corn, but these should not make up more than 10% of their diet.\n",
      "4. Fruits and vegetables: Fresh fruits and vegetables, such as apples, carrots, and sweet potatoes, can be given to llamas as treats or added to their hay.\n",
      "5. Browse: Llamas may also eat browse, which includes leaves, twigs, and other vegetation from trees and shrubs.\n",
      "\n",
      "It's essential to note that llamas have a unique digestive system, with a four-chambered stomach, which allows them to break down and extract nutrients from plant material more efficiently than many other animals. However, this also means they can be prone to certain health issues if their diet is not balanced or if they eat too much of the wrong foods.\n",
      "\n",
      "A good rule of thumb for llama owners is to provide a high-quality hay-based diet with limited amounts of grains and treats, and to ensure access to fresh water at all times.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2:latest\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the entries from our fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset response:\n",
      ">>  The car is as fast as lightning.\n",
      "\n",
      "Model: \n",
      ">>  \n",
      "\n",
      "Score: \n",
      ">>  To complete the request, I will rewrite the input sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "\"The car is very fast.\"\n",
      "\n",
      "### Output:\n",
      "\"The car is as fast as lightning.\"\n",
      "\n",
      "Now, let's evaluate the model response. Since there was no actual input to score, I'll assume it's a generic example. Based on this, I would give the model response a score of 100, as it correctly uses a simile to describe the speed of the car.\n",
      "--------------------------------\n",
      " \n",
      "Dataset response:\n",
      ">>  The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model: \n",
      ">>  What type of clouds?\n",
      "What type of clouds?\n",
      "What type of clouds?\n",
      "What type of clouds?\n",
      "What type of clouds?\n",
      "\n",
      "Score: \n",
      ">>  ### Response:\n",
      "The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "### Score: 100/100\n",
      "\n",
      "This response meets all the requirements:\n",
      "\n",
      "1. It directly answers the question.\n",
      "2. The answer is accurate and relevant to the topic (cumulonimbus clouds are indeed commonly associated with thunderstorms).\n",
      "3. The response is concise and clear, providing a direct answer without unnecessary elaboration.\n",
      "\n",
      "The model's response of \"What type of clouds?\" repeated multiple times does not meet the requirements, as it fails to provide a specific and accurate answer to the question.\n",
      "--------------------------------\n",
      " \n",
      "Dataset response:\n",
      ">>  Jane Austen.\n",
      "\n",
      "Model: \n",
      ">>  \n",
      "\n",
      "Score: \n",
      ">>  The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score: 100\n",
      "--------------------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input '{data.format_instruction_input(entry)}'\"\n",
    "        f\" and the correct output '{entry['output']}', \"\n",
    "        f\"score the model response '{entry['model_response']}'\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score\"\n",
    "    )\n",
    "    print(\"Dataset response:\")\n",
    "    print(\">> \", entry['output'])\n",
    "    print(\"\\nModel: \")\n",
    "    print(\">> \", entry[\"model_response\"])\n",
    "    print(\"\\nScore: \")\n",
    "    print(\">> \", query_model(prompt, model))\n",
    "    print(\"--------------------------------\\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data, json_key):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input '{data.format_instruction_input(entry)}'\"\n",
    "            f\" and the correct output '{entry['output']}', \"\n",
    "            f\"score the model response '{entry['model_response']}'\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score\"\n",
    "            f\" Respon only with a number\"\n",
    "        )\n",
    "        score = query_model(prompt)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"could not convert {score}\")\n",
    "            continue\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   1%|          | 1/110 [00:01<02:56,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert The car is like lightning. \n",
      "\n",
      "Score: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   5%|▌         | 6/110 [00:03<00:44,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert The rewritten sentence is: \"The lecture was delivered clearly.\"\n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  18%|█▊        | 20/110 [00:09<00:43,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert I have traveled without a map. \n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  24%|██▎       | 26/110 [00:12<01:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert I can provide a corrected response.\n",
      "\n",
      "### Instruction:\n",
      "Define the term 'hyperbole'.\n",
      "\n",
      "Hyperbole is a figure of speech that involves an exaggeration of ideas for the sake of emphasis.\n",
      "\n",
      "The term 'bole' on a scale from 0 to 100, where 100 is the best score. 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  29%|██▉       | 32/110 [00:15<00:40,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert It's very easy. \n",
      "\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  37%|███▋      | 41/110 [00:19<00:29,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert The company's approach to product development has been highly innovative. \n",
      "\n",
      "Score: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  41%|████      | 45/110 [00:21<00:24,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert The transient nature of her visit left a lasting impression. \n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  48%|████▊     | 53/110 [00:24<00:25,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert Vehicles: Bicycle\n",
      "Plants: Rose\n",
      "Animals: Tiger\n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  53%|█████▎    | 58/110 [00:26<00:17,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert He will be reading a novel inspired by his grandmother. \n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  55%|█████▍    | 60/110 [00:27<00:15,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert The outcome of this situation was inevitable given the escalating tensions. \n",
      "\n",
      "Score: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  56%|█████▋    | 62/110 [00:28<00:17,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert I can't fulfill this request.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  76%|███████▋  | 84/110 [00:38<00:12,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert He remained very calm. \n",
      "\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  95%|█████████▍| 104/110 [00:48<00:03,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert I can't fulfill this request.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  95%|█████████▌| 105/110 [00:48<00:03,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  99%|█████████▉| 109/110 [00:50<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert She never forgets to call. \n",
      "\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:52<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert I can't provide a response that includes random characters or numbers in the format you requested. However, I can provide the correct model response and score:\n",
      "\n",
      "Correct Model Response:\n",
      "\"50 miles per hour is approximately 80.47 kilometers per hour.\"\n",
      "\n",
      "Score: 100\n",
      "Total scores: 94 of 110\n",
      "Average score: 45.61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Total scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
